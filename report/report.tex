\documentclass[a4paper,12pt]{article}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage{etoolbox}
\usepackage{fullpage}
\renewcommand*\contentsname{Indice}
\renewcommand*\figurename{Fig.}
\usepackage{setspace}
\usepackage{parskip}
\usepackage{subfigure}
\usepackage{amsmath}


\makeatletter
\patchcmd\l@section{%
  \nobreak\hfil\nobreak
}{%
  \nobreak
  \leaders\hbox{%
    $\m@th \mkern \@dotsep mu\hbox{.}\mkern \@dotsep mu$%
  }%
  \hfill
  \nobreak
}{}{\errmessage{\noexpand\l@section could not be patched}}
\makeatother

\setcounter{secnumdepth}{0}

% un po' di estetica...
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\headsep}{0.35in}
\let\MakeUppercase\relax

% blocchi di codice
\usepackage{listings}
\lstset{
	breaklines=true, 
	frame=single, 
	numbers=left,
	tabsize=2,
	basicstyle=\scriptsize,
	showstringspaces=false
}

\setlength{\parindent}{2em}
\setlength{\parskip}{0.5em}
\renewcommand{\baselinestretch}{1.5}

\fancyhf{} % clear all fields
\fancyfoot[C]{\thepage}

\frenchspacing

\begin{document}

\begin{titlepage}
\noindent
    \vspace*{5mm}
	\begin{minipage}[t]{0.15\textwidth}
	    \vspace*{5mm}
		\vspace{-3.5mm}{\includegraphics[scale=1.8]{img/logo_bicocca.png}}
	\end{minipage}
	\hspace{1cm}
	\begin{minipage}[t]{0.9\textwidth}
	      \vspace*{5mm}
		{
			\setstretch{1.42}
			{\textsc{Università degli Studi di Milano - Bicocca} } \\
			\textbf{Scuola di Scienze} \\
			\textbf{Dipartimento di Informatica, Sistemistica e Comunicazione} \\
			\textbf{Corso di Laurea Magistrale in Informatica} \\
			\par
		}
	\end{minipage}
	
	\vspace{42mm}

\begin{center}
    {\LARGE{
	    	\setstretch{2}
            \textbf{
            	Tecniche di Record Linkage \\ }
    }}        
\end{center}

\vspace{40mm}
	
	
	\begin{flushright}
		\setstretch{1.3}
		\large{Alberici Federico - 808058\\} 
		\large{Bettini Ivo Junior - 806878\\} 
		\large{Cocca Umberto - 807191\\} 
		\large{Traversa Silvia - 816435} 
	\end{flushright}
	
	\vspace{15mm}
	\begin{center}
		{\large{\bf Anno Accademico 2019 - 2020}}
	\end{center}


\renewcommand{\baselinestretch}{1.5}

\end{titlepage}

\tableofcontents

\newpage

\section{Ricerca} 

\subsection{Data Quality}

La consapevolezza del peso che dati di alta qualità hanno nel supportare decisioni informate e, viceversa, delle conseguenze disastrose cui dati inaccurati possono portare, è cresciuta di pari passo con il diffondersi delle fonti informative a disposizione delle organizzazioni, creando sempre più forte l’esigenza di una gestione adeguata della qualità dei dati aziendali. \\
La ricerca sulla qualità dei dati è iniziata correttamente negli anni '90 e varie definizioni di ciò sono state definite nel corso degli anni. \\
Un gruppo di ricerca del MIT, guidato da il professor Wang, ha definito la qualità dei dati come condizione per l'uso e ha proposto il suo giudizio dipende dai suoi consumatori. Allo stesso tempo, hanno definito una "dimensione della qualità dei dati" come un insieme di attributi di qualità dei dati che rappresentano un singolo aspetto o costrutto della qualità dei dati. \\
Sono necessarie tecniche di misurazione completa per consentire alle organizzazioni di valutare lo stato della loro qualità delle informazioni organizzative e monitorarne il miglioramento. \\
Ma cosa si intende quando si parla di qualità dei dati e come si misura? \\
Le best practice in questo ambito suggeriscono l’utilizzo di opportune metriche per la definizione e la misurazione della qualità del dato.
Tra le metriche più comuni troviamo:

\begin{itemize}
\item \textbf{completezza}, i dati raccolti bastano per rappresentare l'informazione necessaria; 
\item \textbf{accuratezza}, la precisone dei dati;
\item \textbf{tempestività}, i tempi di acquisizione dei dati sono utili per il processo;
\item \textbf{coerenza}, i dati non sono contradditori tra di loro;
\item \textbf{univocità}, i dati rappresentativi della stessa informazione presenti in diversi componenti del sistema informativo assumono lo stesso valore;
\item \textbf{integrità}, i dati presenti nel sistema informativo corrispondono a quelli originariamente immessi;
\item \textbf{conformità formale}, i dati immessi nel sistema informativo rispettano gli standard formali appositamente definiti.
\end{itemize} 

In tempi attuali, è emerso un altro tipo problema: i big data. Analisi e ricerca complete di standard di qualità e metodi di valutazione della qualità per questo tipo di informazioni attualmente manca o non è completa.

\subsection{Metodoligia Data Quality}

Il professor Batini definisce la metodologia di qualità dei dati come un insieme di linee guida e tecniche che, a partire dalle informazioni di input che descrivono un determinato contesto applicativo, ne deriva un processo razionale per valutare e migliorare la qualità dei dati. Ci sono tre fasi principali per tale attività:
\begin{itemize}
\item \textbf{ricostruzione dello stato}, al fine di ottenere due informazioni contestuali, facoltative se sono già disponibili per l'uso;
\item \textbf{valutazione e misurazione}, misurazione della qualità lungo dimensioni della qualità pertinenti o valutazione, quando tali misurazioni vengono confrontate con i valori di riferimento;
\item \textbf{miglioramento}, attività che mirano per raggiungere nuovi obiettivi di qualità dei dati.
\end{itemize}

\subsection{Miglioramento}

Il miglioramento della qualità dei dati può essere effettuato attraverso strategie basate sui dati o sui processi. Nel primo caso, le tecniche più diffuse sono quella di standardizzazione (o normalizzazione), il record linkage e l'integrazione degli schemi e dei dati, mentre nel secondo caso si adotta un processo di ricostruzione. Nel caso del nostro progetto, per poter migliorare la qualità del dato abbiamo deciso di utilizzare la tecnica del record linkage. %%bisogna trovare una motivazione per spiegare perchè usiamo il record linkage, un motivo per cui l'abbiamo scelta

\subsection{Standardizzazione}

Questo processo, chiamato anche normalizzazione, sostituisce per esempio una diversa ortografia di una parola con una sola ortografia.

\subsection{Comparazione stringhe}

Gli errori tipografici rendono impossibile confrontare esattamente tra di loro le stinghe.  Per poter fare ciò, quindi, serve una funzione che cerca di trovare un punto di accordo tra i dati. Ci sono stati diversi tentativi di fornire questa funzione:

\begin{itemize}
\item Jaro ha proposto un comparatore di stringhe che tiene conto di inserimenti, eliminazioni e trasposizioni necessarie per abbinare le due stringhe;
\item Winkler ha proposto una variante della distanza Jaro (Jaro-Winkler);
\item la distanza q-gram conta il numero di q caratteri consecutivi che concordano tra due corde;
\item la distanza di edit classica, che conta il numero di operazioni (inserimenti, eliminazioni, modi cazioni) necessarie per abbinare le due stringhe
\end{itemize}

\subsection{Record Linkage}

Il record linkage (conosciuto anche come data matching) è l'operazione che consiste nel trovare "records", in dataset che si riferiscono alla stessa entità, presi da differenti risorse (come ad esempio file, libri, siti e database).
Questa operazione diventa necessaria quando vogliamo unire dei dataset differenti basati su dati simili che potrebbero avere o non avere lo stesso identificativo. //
L'idea moderna di record linkage nasce alla fine degli anni cinquanta e viene formalizzata qualche tempo dopo da Ivan Fellegi e Alan Sunter che, attraverso il loro lavoro, hanno dimostrato che le regole di decisione probabilistiche sono ottimali quando i dati che vengono confrontati sono condizionatamente indipendenti. //
A partire dalla fine degli anni novanta, differenti tecniche di machine learning sono state sviluppate per poter capire, con condizioni favorevoli, la probabilità condizionata richiesta dalla teoria Fellegi-Sunter. //
Il record linkage può essere interamente eseguito senza l'aiuto di un computer, ma il motivo principale per cui esso viene utilizzato è perchè si vuole ridurre o eliminare le modifiche "fatte a mano" e per rendere più facile l'ottenimento del risultato. 
L'utilizzo del computer ha anche il vantaggio di utilizzare un processo di supervisione centrale, miglior qualità di controllo, velocità, consistenza e migliore riproducibilità dei risultati. %presa da wiki, un po da edulcorare :)

\subsection{Metodologie di Record Linkage}
% descrivo le varie metodologie, alla fine dovremmo dire in base alla libreria che utilizziamo noi quale metodo abbiamo usato
Il record linkage si divide generalmente nei seguenti step:
\begin{itemize}
\item vengono dati in input dei dataset;
\item viene definito uno spazio iniziale di ricerca;
\item nella fase di "blocking" si cerca di ridurre lo spazio di ricerca;
\item viene definito lo spazio ridotto di ricerca;
\item vengono comporati i dati e viene presa una decisione;
\item viene definito se esiste un matvh, un possibile match o se i dati non sono collegabili
\end{itemize}
%si potrebbe mettere l'immagine a pagina 30 del pdf di batini oppure pagina 36

Data preprocessing %sto modificando da browser, andrebbe messo in grassetto
\\
Il record linkage è molto sensibile alla qualità dei dati che devono essere collegati, quindi idealmente prima di svolgere questa operazione ogni dataset deve essere controllato affinche la qualità sia delle migliori. Avvengono delle operazioni di standardizzazione, che consistono nel trasformare i dati o procedure più complesse come ad esempio la tokenizzazione.\\

Entity resolution % stesso commento di sopra
\\
L'entity resolution è un processo di operazioni intelligenti che permettono alle organizzazioni di connettere i dati più disparati attraverso la possibilità di capire i matches tra le entità e le relazioni non ovvie fra i diversi dati.\\
Essa analizza tutte le informazioni collegate ad una entità prese da diverse sorgenti e cerca attraverso un calcolo probabilistico di determinare quali entità sono collegate e quali collegamenti (non ovvi) esistono fra loro.\\

Deterministic record linkage % stesso commento di sopra
\\
La metodologia più semplice di record linkage è chiamata "deterministica", essa genera collegamenti basati sul numero di singoli identificatori che hanno una corrispondeza fra i dati dei dataset.\\
Due record si dicono "collegati" con una procedura di record linkage deterministico se se tutti o alcuni degli identificatori sono identici.\\
Questo metodo è una buona opzione se si stanno utilizzando dei dataset con delle entità che sono identificate da un id comune.\\
% io non lo metterei, ma vogliamo citare un esempio?? magari quello di batini

Probabilistic record linkage % stesso commento di sopra
\\
Il record linkage probabilistico, chiamato anche fuzzy matching, utilizza un approccio differente per poter collegare i dati. Viene tenuto conto di una gamma più ampia di potenziali identificatori, calcolando i pesi per ciascun identificatore in base alla sua capacità stimata di identificare correttamente una corrispondenza o una non corrispondenza e usando questi pesi per calcolare la probabilità che due dati registrati si riferiscano alla stessa entità.\\
Le coppie di record con probabilità al di sopra di una determinata soglia sono considerate corrispondenze, viceversa le altre sono considerate non corrispondenze. Le coppie che rientrano tra queste due soglie sono considerate "possibili corrispondenze" e possono essere trattate di conseguenza (ad esempio, revisioni umane, collegate o non collegate, a seconda dei requisiti). Mentre il collegamento deterministico dei record richiede una serie di regole potenzialmente complesse da programmare in anticipo, i metodi probabilistici di collegamento dei record possono essere "addestrati" per funzionare bene con un intervento molto meno umano.\\

Machine learning %stesso commento di sopra
\\
Negli ultimi anni, sono state utilizzate varie tecniche di apprendimento automatico nel collegamento discografico. È stato riconosciuto che l'algoritmo classico per il collegamento probabilistico dei record sopra descritto è equivalente all'algoritmo Naive Bayes nel campo dell'apprendimento automatico, e utilizza la stessa assunzione dell'indipendenza delle sue caratteristiche (un presupposto che in genere non è vero). \\
È possibile ottenere una maggiore precisione utilizzando varie altre tecniche di apprendimento automatico, incluso un percettrone a strato singolo. Insieme alle tecnologie distribuite, l'accuratezza e la scala per il collegamento dei record possono essere ulteriormente migliorate.\\

\subsection{Tools usati}

\subsubsection{Python Record Linkage Toolkit }
Python Record Linkage Toolkit è una libreria che permette di effettuare record linkage sia in una sola fonte di dati che in multiple. Il package contiene metodi di indexing, come blocking e sorted neighbourhood indexing, funzioni per il confronto con diverse misure di similarità possibili e diversi algoritmi di classificazione, sia supervisionati che non. 


\section{Esperimenti}
\subsection{Dataset}
Il dataset su cui sono stati effettuati gli esperimenti è un elenchi di ristoranti di Manhattan, estratti settimanalmente da Gennaio a Marzo 2009 da 12 siti web. Le informazioni base presenti in ogni dataset comprendono il nome del ristorante, l'indirizzo e la città.
I dati sono messi a disposizione in sette file di testo, uno per ogni settimana considerata, ciascuno contenente dati appartenenti a tutti i siti web. In totale sono presenti 215555 record, suddivisi come illustrato nella tabella:
\begin{table}[H] \centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{file}} & \multicolumn{1}{c|}{\textbf{records}} \\ \hline
restaurants\_2009\_1\_22.txt & 30401 \\ \hline
restaurants\_2009\_1\_29.txt & 30775 \\ \hline
restaurants\_2009\_2\_05.txt & 30805 \\ \hline
restaurants\_2009\_2\_12.txt & 30863 \\ \hline
restaurants\_2009\_2\_19.txt & 30876 \\ \hline
restaurants\_2009\_2\_26.txt & 30898 \\ \hline
restaurants\_2009\_3\_12.txt & 30937 \\ \hline
\end{tabular}
\caption{Record presenti nei file txt forniti}
\label{tab:Tab}
\end{table}

In particolare, per ogni ristorante è presente il seguente numero di record:

\begin{table}[H]\centering
\begin{tabular}{|c|c|}
\hline
\textbf{restaurant} & \textbf{records} \\ \hline
ActiveDiner & 6184 \\ \hline
DiningGuide & 814 \\ \hline
FoodBuzz & 2079 \\ \hline
MenuPages & 13143 \\ \hline
NewYork & 1774 \\ \hline
NYMag & 5124 \\ \hline
NYTimes & 3095 \\ \hline
OpenTable & 1539 \\ \hline
SavoryCities & 4536 \\ \hline
TasteSpace & 3635 \\ \hline
TimeOut & 14007 \\ \hline
VillageVoice & 2684 \\ \hline
\end{tabular}
\caption{Record per ristorante}
\label{tab:my-table}
\end{table}
\subsection{Data preprocessing}
Una prima analisi mostra come le modalità di recupero dei dati da parte dell'autore abbiano generato dei dataset differenti per schema e per frammentazione verticale, anche sulle stesse fonti. Dunque, prima di applicare le tecniche di record linkage, i dati sono stati standardizzati, eseguendo operazioni di riallineamento dello schema, rimozione dei duplicati ed infine join dei dataset per fonte.
\subsubsection{Risultati}
%info sui dati puliti

\subsection{Record Linkage}

\subsection{Gold Standard}
%possiamo vedere il gold standard e confrontarlo con i nostri risultati
\end{document}
